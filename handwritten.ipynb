{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4eac49-3f86-4be5-801b-fa4845708dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "# Read the data...\n",
    "data = pd.read_csv(r\"/home/praveen/Downloads/Engineering/sem6/Project/handwritten-character-recognition-code/A_Z Handwritten Data.csv\").astype('float32')\n",
    "\n",
    "# Split data the X - Our data , and y - the prdict label\n",
    "X = data.drop('0',axis = 1)\n",
    "y = data['0']\n",
    "\n",
    "\n",
    "# Reshaping the data in csv file so that it can be displayed as an image...\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2)\n",
    "train_x = np.reshape(train_x.values, (train_x.shape[0], 28,28))\n",
    "test_x = np.reshape(test_x.values, (test_x.shape[0], 28,28))\n",
    "\n",
    "\n",
    "print(\"Train data shape: \", train_x.shape)\n",
    "print(\"Test data shape: \", test_x.shape)\n",
    "\n",
    "# Dictionary for getting characters from index values...\n",
    "word_dict = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'}\n",
    "\n",
    "\n",
    "# Plotting the number of alphabets in the dataset...\n",
    "\n",
    "train_yint = np.int0(y)\n",
    "count = np.zeros(26, dtype='int')\n",
    "for i in train_yint:\n",
    "    count[i] +=1\n",
    "\n",
    "alphabets = []\n",
    "for i in word_dict.values():\n",
    "    alphabets.append(i)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "ax.barh(alphabets, count)\n",
    "\n",
    "plt.xlabel(\"Number of elements \")\n",
    "plt.ylabel(\"Alphabets\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Shuffling the data ...\n",
    "shuff = shuffle(train_x[:100])\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (10,10))\n",
    "axes = ax.flatten()\n",
    "\n",
    "for i in range(9):\n",
    "    axes[i].imshow(np.reshape(shuff[i], (28,28)), cmap=\"Greys\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Reshaping the training & test dataset so that it can be put in the model...\n",
    "\n",
    "train_X = train_x.reshape(train_x.shape[0],train_x.shape[1],train_x.shape[2],1)\n",
    "print(\"New shape of train data: \", train_X.shape)\n",
    "\n",
    "test_X = test_x.reshape(test_x.shape[0], test_x.shape[1], test_x.shape[2],1)\n",
    "print(\"New shape of train data: \", test_X.shape)\n",
    "\n",
    "\n",
    "# Converting the labels to categorical values...\n",
    "\n",
    "train_yOHE = to_categorical(train_y, num_classes = 26, dtype='int')\n",
    "print(\"New shape of train labels: \", train_yOHE.shape)\n",
    "\n",
    "test_yOHE = to_categorical(test_y, num_classes = 26, dtype='int')\n",
    "print(\"New shape of test labels: \", test_yOHE.shape)\n",
    "\n",
    "\n",
    "# CNN model...\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "\n",
    "model.add(Dense(26,activation =\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history = model.fit(train_X, train_yOHE, epochs=1, callbacks=[reduce_lr, early_stop],  validation_data = (test_X,test_yOHE))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.save(r'model_hand.h5')\n",
    "\n",
    "\n",
    "# Displaying the accuracies & losses for train & validation set...\n",
    "\n",
    "print(\"The validation accuracy is :\", history.history['val_accuracy'])\n",
    "print(\"The training accuracy is :\", history.history['accuracy'])\n",
    "print(\"The validation loss is :\", history.history['val_loss'])\n",
    "print(\"The training loss is :\", history.history['loss'])\n",
    "\n",
    "\n",
    "\n",
    "#Making model predictions...\n",
    "\n",
    "pred = model.predict(test_X[:9])\n",
    "print(test_X.shape)\n",
    "\n",
    "\n",
    "# Displaying some of the test images & their predicted labels...\n",
    "\n",
    "fig, axes = plt.subplots(3,3, figsize=(8,9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    img = np.reshape(test_X[i], (28,28))\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    pred = word_dict[np.argmax(test_yOHE[i])]\n",
    "    ax.set_title(\"Prediction: \"+pred)\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afed999-cbbe-4d79-8aed-1e76df55216e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-883d80b1d700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mimg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dataflair _ _ _ \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_TRIPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m230\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Prediction on external image...\n",
    "\n",
    "img = cv2.imread(r'/home/praveen/Downloads/Engineering/sem6/Project/f.jpg')\n",
    "img_copy = img.copy()\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (400,440))\n",
    "\n",
    "img_copy = cv2.GaussianBlur(img_copy, (7,7), 0)\n",
    "img_gray = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
    "_, img_thresh = cv2.threshold(img_gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "img_final = cv2.resize(img_thresh, (28,28))\n",
    "img_final =np.reshape(img_final, (1,28,28,1))\n",
    "\n",
    "\n",
    "img_pred = word_dict[np.argmax(model.predict(img_final))]\n",
    "\n",
    "cv2.putText(img, \"Dataflair _ _ _ \", (20,25), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color = (0,0,230))\n",
    "cv2.putText(img, \"Prediction: \" + img_pred, (20,410), cv2.FONT_HERSHEY_DUPLEX, 1.3, color = (255,0,30))\n",
    "cv2.imshow('Dataflair handwritten character recognition _ _ _ ', img)\n",
    "\n",
    "\n",
    "while (1):\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017583cb-1e77-4bab-8e38-a4fd5ec42724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
